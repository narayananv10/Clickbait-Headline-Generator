{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZlxWi7kohmRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a735268-19d7-478a-a47c-4d65e2a888d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('clickbait_data.gz',sep='\\t',names=['clickbait'], header=None)"
      ],
      "metadata": {
        "id": "VCpoCnBK0_FL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wwwIrPMn5EC4",
        "outputId": "05a85892-571d-4645-d4f5-d6f1e81121f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           clickbait\n",
              "0                                 Should I Get Bings\n",
              "1      Which TV Female Friend Group Do You Belong In\n",
              "2  The New \"Star Wars: The Force Awakens\" Trailer...\n",
              "3  This Vine Of New York On \"Celebrity Big Brothe...\n",
              "4  A Couple Did A Stunning Photo Shoot With Their..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a1e5709-c5f4-4b8c-b08c-15a697d4dda0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clickbait</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Should I Get Bings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a1e5709-c5f4-4b8c-b08c-15a697d4dda0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a1e5709-c5f4-4b8c-b08c-15a697d4dda0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a1e5709-c5f4-4b8c-b08c-15a697d4dda0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_headlines(headlines):\n",
        "    headlines = [i for i in headlines if i.isalpha() or i.isspace()] \n",
        "    headlines = ''.join(headlines)\n",
        "    headlines = headlines.lower()\n",
        "    return headlines\n",
        "\n",
        "df['clickbait'] = df['clickbait'].apply(preprocess_headlines)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Oi3edldcQdOx",
        "outputId": "742de976-3ad0-4afa-a0fb-df77d2af039d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               clickbait\n",
              "0                                     should i get bings\n",
              "1          which tv female friend group do you belong in\n",
              "2      the new star wars the force awakens trailer is...\n",
              "3      this vine of new york on celebrity big brother...\n",
              "4      a couple did a stunning photo shoot with their...\n",
              "...                                                  ...\n",
              "15994  there was a mini sisterhood of the traveling p...\n",
              "15995       dogs who are thankful for their best friends\n",
              "15996  people are proving no dick is too big by dropp...\n",
              "15997                           im an atheist but im not\n",
              "15998  an artist drew disney men as justin bieber and...\n",
              "\n",
              "[15999 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4eef551-a9d8-4e2b-8147-2982cbda44af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clickbait</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>should i get bings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which tv female friend group do you belong in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the new star wars the force awakens trailer is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this vine of new york on celebrity big brother...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a couple did a stunning photo shoot with their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15994</th>\n",
              "      <td>there was a mini sisterhood of the traveling p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15995</th>\n",
              "      <td>dogs who are thankful for their best friends</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15996</th>\n",
              "      <td>people are proving no dick is too big by dropp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15997</th>\n",
              "      <td>im an atheist but im not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15998</th>\n",
              "      <td>an artist drew disney men as justin bieber and...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15999 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4eef551-a9d8-4e2b-8147-2982cbda44af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b4eef551-a9d8-4e2b-8147-2982cbda44af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b4eef551-a9d8-4e2b-8147-2982cbda44af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "\n",
        "# Define the tokenizer\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "# Tokenize the headlines\n",
        "tokenized_headlines = [tokenizer(headline) for headline in df.clickbait]\n",
        "\n",
        "word_counts = Counter()\n",
        "for headline in tokenized_headlines:\n",
        "    word_counts.update(headline)\n",
        "\n",
        "# Create a dictionary that maps each word to a unique index\n",
        "word_to_idx = {}\n",
        "for i, (word, count) in enumerate(word_counts.items()):\n",
        "    word_to_idx[word] = i\n",
        "    \n",
        "# Build the vocabulary\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_headlines)\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "# Convert the tokenized headlines to numerical sequences\n",
        "numerical_headlines = [[word_to_idx[token] for token in headline] for headline in tokenized_headlines]"
      ],
      "metadata": {
        "id": "WYZBY27oMwcb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert the numerical headlines to PyTorch tensors\n",
        "tensor_headlines = [torch.tensor(headline) for headline in numerical_headlines]\n",
        "\n",
        "# Pad the tensors to the same length\n",
        "padded_headlines = torch.nn.utils.rnn.pad_sequence(tensor_headlines, batch_first=True)\n",
        "\n",
        "# Split the padded tensors into input and target sequences\n",
        "input_seqs = padded_headlines[:, :-1]\n",
        "target_seqs = padded_headlines[:, 1:]\n",
        "\n",
        "# Create a dataset from the input and target sequences\n",
        "dataset = TensorDataset(input_seqs, target_seqs)\n",
        "\n",
        "max_seq_length = 100\n",
        "pad_idx = 0\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    inputs = [torch.tensor(item[0]) for item in batch]\n",
        "    labels = [torch.tensor(item[1]) for item in batch]\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_idx)\n",
        "    labels = pad_sequence(labels, batch_first=True, padding_value=pad_idx)\n",
        "    return inputs, labels\n",
        "\n",
        "# Define data loaders for the dataset\n",
        "batch_size = 32\n",
        "train_size = int(0.7 * len(dataset))\n",
        "eval_size = len(dataset) - train_size\n",
        "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "Z7QMdJC2M_m-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, embedding_size, max_seq_length):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        embedded = self.embedding(inputs)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        logits = self.fc(output)\n",
        "        return output.reshape(-1, output.size(2)), hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.num_layers, batch_size, self.hidden_size).zero_()\n",
        "        cell = weight.new(self.num_layers, batch_size, self.hidden_size).zero_()\n",
        "        return hidden, cell\n"
      ],
      "metadata": {
        "id": "DKLOxmKoiR_y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOGh2YLljBJr",
        "outputId": "e12bc2e7-ec81-454f-e749-4e5060e57050"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12184"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_sequence(seq, max_len):\n",
        "    \"\"\"\n",
        "    Truncates a sequence to a maximum length.\n",
        "    \"\"\"\n",
        "    if len(seq) > max_len:\n",
        "        return seq[:max_len]\n",
        "    else:\n",
        "        return seq"
      ],
      "metadata": {
        "id": "lShakxYCo8Co"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 12184\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "max_seq_length = 100\n",
        "model = cLSTM(vocab_size, hidden_size=hidden_size, num_layers=num_layers, embedding_size=embed_size, max_seq_length=max_seq_length)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Initialize hidden state\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Loop over training set\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "      # truncate the input sequence if it is longer than the maximum length\n",
        "      if inputs.shape[1] > max_seq_length:\n",
        "        inputs = truncate_sequence(inputs, max_seq_length)\n",
        "\n",
        "      # Initialize hidden state\n",
        "      hidden = model.init_hidden(batch_size)\n",
        "\n",
        "      # Zero out gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output, hidden = model(inputs, hidden)\n",
        "      output_flat = output.view(-1, output.shape[-1])\n",
        "      labels_flat = labels.view(-1)\n",
        "      loss = criterion(output_flat, labels_flat)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print statistics\n",
        "      if i % print_every == 0:\n",
        "          print(f\"Epoch {epoch+1}/{num_epochs}, Step {i+1}/{num_batches}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    \n",
        "    # Set evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_batches = 0\n",
        "        for inputs, labels in eval_loader:\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          output_flat = output.view(-1, output.shape[-1])\n",
        "          labels_flat = labels.view(-1) \n",
        "          loss = criterion(output_flat, labels_flat)\n",
        "          \n",
        "          # Update total loss and batches\n",
        "          total_loss += loss.item() * inputs.size(0)\n",
        "          total_batches += inputs.size(0)\n",
        "        \n",
        "        # Calculate average loss\n",
        "        avg_loss = total_loss / total_batches\n",
        "        print(f'Test Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "JNBLdlDZV0b3",
        "outputId": "7ddd06ea-3bb0-4efb-b822-b7698b228be5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-80e659ee009f>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = [torch.tensor(item[0]) for item in batch]\n",
            "<ipython-input-6-80e659ee009f>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(item[1]) for item in batch]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fddae25854b1>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0moutput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mlabels_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 4060 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Set the seed text\n",
        "seed_text = \"Artificial Intelligence\"\n",
        "\n",
        "# Set the number of words to generate\n",
        "num_words = 10\n",
        "\n",
        "# Convert the seed text to a tensor\n",
        "input_tensor = torch.LongTensor([[word_to_index.get(word, 0) for word in seed_text.split()]])\n",
        "\n",
        "# Generate new headlines\n",
        "for i in range(num_words):\n",
        "    # Feed the input tensor to the model to get the output logits\n",
        "    with torch.no_grad():\n",
        "        output, _, _ = model(input_tensor)\n",
        "    \n",
        "    # Get the last predicted index of the next word\n",
        "    logits = output[0, -1]\n",
        "    index = torch.argmax(F.softmax(logits, dim=0)).item()\n",
        "    \n",
        "    # Convert the predicted index to its corresponding word and add it to the input text\n",
        "    predicted_word = index_to_word.get(index, '')\n",
        "    seed_text += ' ' + predicted_word\n",
        "    \n",
        "    # Convert the updated input text to a tensor for the next iteration\n",
        "    input_tensor = torch.LongTensor([[word_to_index.get(word, 0) for word in seed_text.split()]])\n",
        "    \n",
        "print(seed_text)\n"
      ],
      "metadata": {
        "id": "D7STCdFQeJWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk4U7LmLv6R6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}